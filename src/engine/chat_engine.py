from typing import List, Optional, AsyncGenerator, Tuple
from enum import Enum
from dataclasses import dataclass

from llama_index.core import VectorStoreIndex
from llama_index.core.chat_engine import CondensePlusContextChatEngine
from llama_index.core.memory import ChatMemoryBuffer
from llama_index.core.llms import ChatMessage, MessageRole
from llama_index.core.schema import TextNode, NodeWithScore

from src.config import settings
from src.engine.components import get_llm, get_embed_model, get_reranker, get_vector_store
from src.engine.retriever import HybridRetrieverFactory


# =============================================================================
# INTENT TYPES
# =============================================================================

class IntentType(str, Enum):
    CHAT = "CHAT"  # ChÃ o há»i, há»i chung
    LAW = "LAW"    # CÃ¢u há»i phÃ¡p luáº­t 


@dataclass
class RouterResult:
    intent: IntentType
    confidence: float
    reasoning: str


# =============================================================================
# PROMPTS
# =============================================================================

# Prompt phÃ¢n loáº¡i intent
ROUTER_PROMPT = """Báº¡n lÃ  bá»™ phÃ¢n loáº¡i Ã½ Ä‘á»‹nh. XÃ¡c Ä‘á»‹nh cÃ¢u há»i thuá»™c loáº¡i nÃ o:
1. **LAW**: CÃ¢u há»i vá» phÃ¡p luáº­t lao Ä‘á»™ng Viá»‡t Nam
2. **CHAT**: ChÃ o há»i, cÃ¢u há»i chung khÃ´ng liÃªn quan Ä‘áº¿n luáº­t

QUAN TRá»ŒNG: Náº¿u cÃ³ lá»‹ch sá»­ vá» phÃ¡p luáº­t vÃ  cÃ¢u há»i hiá»‡n táº¡i lÃ  follow-up, phÃ¢n loáº¡i lÃ  LAW.

Lá»‹ch sá»­: {chat_history}
CÃ¢u há»i: {query}

Tráº£ lá»i:
INTENT: [LAW hoáº·c CHAT]
CONFIDENCE: [0.0-1.0]
REASONING: [Giáº£i thÃ­ch ngáº¯n]"""

# Prompt rewrite cÃ¢u há»i follow-up thÃ nh cÃ¢u Ä‘á»™c láº­p
CONDENSE_PROMPT = """Cho lá»‹ch sá»­ vÃ  cÃ¢u há»i tiáº¿p theo, viáº¿t láº¡i thÃ nh cÃ¢u há»i Ä‘á»™c láº­p.

QUY Táº®C QUAN TRá»ŒNG:
1. PHáº¢I GIá»® NGUYÃŠN cÃ¡c tá»« khÃ³a phÃ¡p lÃ½: sÃ¡p nháº­p, tÃ¡i cÆ¡ cáº¥u, mang thai, thai sáº£n, nghá»‰ hÆ°u, sa tháº£i, Ä‘á»™c háº¡i, BHTN, BHXH, trá»£ cáº¥p, há»£p Ä‘á»“ng
2. PHáº¢I GIá»® NGUYÃŠN cÃ¡c con sá»‘: sá»‘ nÄƒm lÃ m viá»‡c, sá»‘ tiá»n lÆ°Æ¡ng, tuá»•i, thá»i gian Ä‘Ã³ng báº£o hiá»ƒm
3. PHáº¢I GIá»® NGUYÃŠN lÃ½ do nghá»‰ viá»‡c náº¿u cÃ³ Ä‘á» cáº­p
4. Chá»‰ viáº¿t láº¡i Ä‘á»ƒ cÃ¢u há»i rÃµ rÃ ng hÆ¡n, KHÃ”NG THAY Äá»”I Ã½ nghÄ©a

Lá»‹ch sá»­: {chat_history}
CÃ¢u há»i: {question}

CÃ¢u há»i Ä‘Ã£ viáº¿t láº¡i:"""

# Prompt hÆ°á»›ng dáº«n LLM tráº£ lá»i dá»±a trÃªn context phÃ¡p luáº­t
CONTEXT_PROMPT = """Báº¡n lÃ  trá»£ lÃ½ AI chuyÃªn gia vá» PhÃ¡p luáº­t Lao Ä‘á»™ng Viá»‡t Nam.

CÆ  Sá» Dá»® LIá»†U: Bá»™ luáº­t Lao Ä‘á»™ng 2019, Luáº­t ATVSLÄ 2015, Luáº­t BHXH 2024, Luáº­t Viá»‡c lÃ m 2024, NÄ145/2020, NÄ12/2022, NÄ293/2025

NGUYÃŠN Táº®C TRáº¢ Lá»œI:
1. Chá»‰ dá»±a vÃ o Ä‘iá»u khoáº£n Ä‘Æ°á»£c cung cáº¥p
2. TrÃ­ch dáº«n tÃªn vÄƒn báº£n, sá»‘ Äiá»u, Khoáº£n
3. Tráº£ lá»i tiáº¿ng Viá»‡t, rÃµ rÃ ng, ngáº¯n gá»n
4. Náº¿u khÃ´ng cÃ³ thÃ´ng tin, tráº£ lá»i 'CÃ¢u há»i cá»§a báº¡n khÃ´ng náº±m trong pháº¡m vi cá»§a tÃ´i.'

QUY Táº®C TÃNH TOÃN TRá»¢ Cáº¤P (náº¿u há»i vá» trá»£ cáº¥p):
- Trá»£ cáº¥p THÃ”I VIá»†C (Äiá»u 46): 0.5 thÃ¡ng lÆ°Æ¡ng Ã— sá»‘ nÄƒm = khi tá»± nghá»‰, háº¿t háº¡n HÄ
- Trá»£ cáº¥p Máº¤T VIá»†C LÃ€M (Äiá»u 47): 1 thÃ¡ng lÆ°Æ¡ng Ã— sá»‘ nÄƒm, tá»‘i thiá»ƒu 2 thÃ¡ng = khi sÃ¡p nháº­p, tÃ¡i cÆ¡ cáº¥u, cáº¯t giáº£m
- Thá»i gian tÃ­nh = Tá»•ng thá»i gian lÃ m viá»‡c - Thá»i gian Ä‘Ã³ng BHTN
- LÃ m trÃ²n: dÆ°á»›i 6 thÃ¡ng â†’ 0.5 nÄƒm, tá»« 6 thÃ¡ng â†’ 1 nÄƒm
- Trá»£ cáº¥p tháº¥t nghiá»‡p (Äiá»u 50 Luáº­t VL): 60% lÆ°Æ¡ng Ã— sá»‘ thÃ¡ng (má»—i 12 thÃ¡ng Ä‘Ã³ng = 3 thÃ¡ng hÆ°á»Ÿng)

CÃC ÄIá»€U KHOáº¢N:
{context_str}"""

# Prompt cho CHAT intent (khÃ´ng cáº§n RAG)
CHAT_RESPONSE_PROMPT = """Báº¡n lÃ  trá»£ lÃ½ AI thÃ¢n thiá»‡n vá» PhÃ¡p luáº­t Lao Ä‘á»™ng Viá»‡t Nam. Tráº£ lá»i cÃ¢u há»i chung hoáº·c chÃ o há»i.

Náº¿u há»i vá» kháº£ nÄƒng, giáº£i thÃ­ch báº¡n cÃ³ thá»ƒ tráº£ lá»i vá» Bá»™ luáº­t Lao Ä‘á»™ng, Luáº­t BHXH, há»£p Ä‘á»“ng lao Ä‘á»™ng, tiá»n lÆ°Æ¡ng, v.v.

Lá»‹ch sá»­: {chat_history}
CÃ¢u há»i: {query}
Tráº£ lá»i:"""


# =============================================================================
# SEMANTIC ROUTER - PhÃ¢n loáº¡i intent LAW/CHAT
# =============================================================================

class SemanticRouter:
    def __init__(self, llm=None):
        self.llm = llm or get_llm()
    
    def _parse_response(self, response_text: str) -> RouterResult:
        try:
            lines = response_text.strip().split('\n')
            intent_line = next((l for l in lines if l.startswith('INTENT:')), None)
            confidence_line = next((l for l in lines if l.startswith('CONFIDENCE:')), None)
            reasoning_line = next((l for l in lines if l.startswith('REASONING:')), None)
            
            intent_str = intent_line.split(':')[1].strip().upper() if intent_line else "LAW"
            confidence = float(confidence_line.split(':')[1].strip()) if confidence_line else 0.8
            reasoning = reasoning_line.split(':', 1)[1].strip() if reasoning_line else ""
            
            return RouterResult(
                intent=IntentType.LAW if intent_str == "LAW" else IntentType.CHAT,
                confidence=confidence,
                reasoning=reasoning
            )
        except Exception:
            return RouterResult(intent=IntentType.LAW, confidence=0.5, reasoning="Parse error")
    
    def route(self, query: str, chat_history: str = "") -> RouterResult:
        """Sync routing"""
        prompt = ROUTER_PROMPT.format(query=query, chat_history=chat_history or "(ChÆ°a cÃ³)")
        response = self.llm.complete(prompt)
        return self._parse_response(str(response))
    
    async def aroute(self, query: str, chat_history: str = "") -> RouterResult:
        """Async routing"""
        prompt = ROUTER_PROMPT.format(query=query, chat_history=chat_history or "(ChÆ°a cÃ³)")
        response = await self.llm.acomplete(prompt)
        return self._parse_response(str(response))


# =============================================================================
# CHAT ENGINE MANAGER - Quáº£n lÃ½ toÃ n bá»™ RAG pipeline
# =============================================================================

class ChatEngineManager:
    """
    Luá»“ng xá»­ lÃ½:
    1. Router phÃ¢n loáº¡i intent (LAW/CHAT)
    2. Náº¿u CHAT â†’ LLM tráº£ lá»i trá»±c tiáº¿p
    3. Náº¿u LAW â†’ Hybrid Search â†’ Rerank â†’ LLM generate vá»›i context
    """
    
    def __init__(self, nodes: Optional[List[TextNode]] = None):
        self.nodes = nodes or []
        self.memory_token_limit = settings.MEMORY_TOKEN_LIMIT        
        self.llm = None
        self.embed_model = None
        self.reranker = None
        self.router = None
        self.hybrid_retriever = None
        self.chat_engine = None
        self.memory = None
        self.vector_store = None
        self._initialized = False
    
    def initialize(self, nodes: Optional[List[TextNode]] = None):
        """Khá»Ÿi táº¡o táº¥t cáº£ components"""
        if nodes:
            self.nodes = nodes
        
        print("ğŸš€ Initializing Chat Engine Manager...")
        
        # Load models
        print("[1/6] Loading LLM...")
        self.llm = get_llm()
        
        print("[2/6] Loading embedding model...")
        self.embed_model = get_embed_model()
        
        print("[3/6] Loading reranker...")
        self.reranker = get_reranker()
        
        # Router Ä‘á»ƒ phÃ¢n loáº¡i intent
        print("[4/6] Initializing semantic router...")
        self.router = SemanticRouter(self.llm)
        
        # Vector store + Hybrid retriever
        print("[5/6] Creating vector index and hybrid retriever...")
        self.vector_store = get_vector_store()
        index = VectorStoreIndex.from_vector_store(
            vector_store=self.vector_store,
            embed_model=self.embed_model,
        )
        
        if self.nodes:
            # Hybrid = Vector + BM25 + RRF fusion
            self.hybrid_retriever = HybridRetrieverFactory.create_from_index(index=index, nodes=self.nodes)
        else:
            print("âš ï¸ No nodes provided, using vector-only retrieval")
            self.hybrid_retriever = index.as_retriever(similarity_top_k=settings.VECTOR_TOP_K)
        
        # Chat engine vá»›i memory
        print("[6/6] Creating chat engine with memory...")
        self.memory = ChatMemoryBuffer.from_defaults(token_limit=self.memory_token_limit)
        
        self.chat_engine = CondensePlusContextChatEngine.from_defaults(
            retriever=self.hybrid_retriever,
            llm=self.llm,
            memory=self.memory,
            node_postprocessors=[self.reranker] if self.reranker else None,
            context_prompt=CONTEXT_PROMPT,
            condense_prompt=CONDENSE_PROMPT,
            verbose=True,
        )
        
        self._initialized = True
        print("âœ… Chat Engine Manager initialized successfully!")
    
    def _ensure_initialized(self):
        if not self._initialized:
            raise RuntimeError("ChatEngineManager not initialized. Call initialize() first.")
    
    def reset(self):
        """Reset conversation memory"""
        self._ensure_initialized()
        self.memory.reset()
    
    def _get_recent_history(self, max_turns: int = 3) -> str:
        """Láº¥y lá»‹ch sá»­ gáº§n Ä‘Ã¢y Ä‘á»ƒ router cÃ³ context"""
        try:
            messages = self.memory.get_all()
            if not messages:
                return ""
            recent = messages[-(max_turns * 2):]
            lines = []
            for msg in recent:
                role = "NgÆ°á»i dÃ¹ng" if msg.role == MessageRole.USER else "Trá»£ lÃ½"
                content = msg.content[:200] + "..." if len(msg.content) > 200 else msg.content
                lines.append(f"{role}: {content}")
            return "\n".join(lines)
        except Exception:
            return ""
    
    def _handle_chat_intent(self, query: str, chat_history: str = "") -> str:
        """Xá»­ lÃ½ CHAT intent - khÃ´ng cáº§n RAG"""
        prompt = CHAT_RESPONSE_PROMPT.format(query=query, chat_history=chat_history or "(ChÆ°a cÃ³)")
        response = self.llm.complete(prompt)
        return response.text if hasattr(response, 'text') else str(response)
    
    async def _ahandle_chat_intent(self, query: str, chat_history: str = "") -> str:
        """Async version"""
        prompt = CHAT_RESPONSE_PROMPT.format(query=query, chat_history=chat_history or "(ChÆ°a cÃ³)")
        response = await self.llm.acomplete(prompt)
        return response.text if hasattr(response, 'text') else str(response)
    
    # =========================================================================
    # MAIN CHAT METHODS
    # =========================================================================
    
    def chat(self, query: str, skip_routing: bool = False) -> Tuple[str, IntentType, List[NodeWithScore]]:
        """
        Sync chat method
        Returns: (response_text, intent, source_nodes)
        """
        self._ensure_initialized()
        source_nodes = []
        
        # Step 1: Route intent
        if skip_routing:
            intent = IntentType.LAW
        else:
            router_result = self.router.route(query, self._get_recent_history())
            intent = router_result.intent
            print(f"ğŸ¯ Router: {intent.value} (confidence: {router_result.confidence:.2f})")
        
        # Step 2: Handle based on intent
        if intent == IntentType.CHAT:
            response_text = self._handle_chat_intent(query, self._get_recent_history())
            self.memory.put(ChatMessage(role=MessageRole.USER, content=query))
            self.memory.put(ChatMessage(role=MessageRole.ASSISTANT, content=response_text or ""))
        else:
            # LAW intent â†’ Use RAG chat engine
            response = self.chat_engine.chat(query)
            response_text = str(response) if response else ""
            source_nodes = response.source_nodes if hasattr(response, 'source_nodes') else []
        
        return response_text or "Xin lá»—i, tÃ´i khÃ´ng thá»ƒ táº¡o cÃ¢u tráº£ lá»i.", intent, source_nodes
    
    async def achat(self, query: str, skip_routing: bool = False) -> Tuple[str, IntentType, List[NodeWithScore]]:
        """Async chat method"""
        self._ensure_initialized()
        source_nodes = []
        
        if skip_routing:
            intent = IntentType.LAW
        else:
            router_result = await self.router.aroute(query, self._get_recent_history())
            intent = router_result.intent
            print(f"ğŸ¯ Router: {intent.value} (confidence: {router_result.confidence:.2f})")

        if intent == IntentType.CHAT:
            response_text = await self._ahandle_chat_intent(query, self._get_recent_history())
            self.memory.put(ChatMessage(role=MessageRole.USER, content=query))
            self.memory.put(ChatMessage(role=MessageRole.ASSISTANT, content=response_text or ""))
        else:
            response = await self.chat_engine.achat(query)
            response_text = str(response) if response else ""
            source_nodes = response.source_nodes if hasattr(response, 'source_nodes') else []
        
        return response_text or "Xin lá»—i, tÃ´i khÃ´ng thá»ƒ táº¡o cÃ¢u tráº£ lá»i.", intent, source_nodes
    
    async def astream_chat(
        self, query: str, skip_routing: bool = False
    ) -> AsyncGenerator[Tuple[str, Optional[IntentType], Optional[List[NodeWithScore]]], None]:
        """
        Streaming chat - yield tá»«ng chunk text
        Cuá»‘i cÃ¹ng yield intent vÃ  source_nodes
        """
        self._ensure_initialized()
        
        # Route intent
        if skip_routing:
            intent = IntentType.LAW
        else:
            router_result = await self.router.aroute(query, self._get_recent_history())
            intent = router_result.intent
            print(f"ğŸ¯ Router: {intent.value} (confidence: {router_result.confidence:.2f})")
        
        if intent == IntentType.CHAT:
            # Stream tá»« LLM trá»±c tiáº¿p
            chat_history = self._get_recent_history() or "(ChÆ°a cÃ³)"
            prompt = CHAT_RESPONSE_PROMPT.format(query=query, chat_history=chat_history)
            full_response = ""
            
            async for chunk in await self.llm.astream_complete(prompt):
                chunk_text = chunk.delta if hasattr(chunk, 'delta') else str(chunk)
                full_response += chunk_text
                yield chunk_text, None, None
            
            self.memory.put(ChatMessage(role=MessageRole.USER, content=query))
            self.memory.put(ChatMessage(role=MessageRole.ASSISTANT, content=full_response))
            yield "", intent, []
        else:
            # Stream tá»« RAG chat engine
            streaming_response = await self.chat_engine.astream_chat(query)
            source_nodes = []
            
            async for chunk in streaming_response.async_response_gen():
                yield chunk, None, None
            
            if hasattr(streaming_response, 'source_nodes'):
                source_nodes = streaming_response.source_nodes
            yield "", intent, source_nodes


# =============================================================================
# SINGLETON PATTERN
# =============================================================================

_chat_engine_manager: Optional[ChatEngineManager] = None


def get_chat_engine_manager() -> ChatEngineManager:
    global _chat_engine_manager
    if _chat_engine_manager is None:
        _chat_engine_manager = ChatEngineManager()
    return _chat_engine_manager


def set_chat_engine_manager(manager: ChatEngineManager):
    global _chat_engine_manager
    _chat_engine_manager = manager
